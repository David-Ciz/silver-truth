{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Silver Truth Project Documentation Introduction Welcome to the documentation for the Silver Truth project. This project focuses on generating and evaluating \"silver-standard\" corpora for cell tracking challenges. A Silver-standard corpus (silver truth) is defined as computer-generated reference annotations, obtained as the majority opinion over the results of several competitive algorithms submitted by former challenge participants. This corpus serves as a robust benchmark for evaluating new cell tracking algorithms. Project Goal This project aims to create a new, better silver truth using more advanced techniques of Quality Assurance and Ensemble methods. Key Components & Workflow The Silver Truth project is structured around a series of command-line tools that facilitate the entire process from data preparation to evaluation. High-Level Workflow Diagram Raw Data -> Synchronization -> DataFrame -> Job Files -> Fusion -> Evaluation Core Modules cli_preprocessing.py : Handles initial data preparation, including synchronizing datasets and creating structured dataframes. cli_fusion.py : Manages the generation of job files and the execution of the cell segmentation fusion process to create the silver truth. cli_evaluation.py : Provides tools for evaluating competitor algorithms against ground truth or the generated silver truth. Getting Started For detailed installation instructions and basic usage examples, please refer to the main README.md file in the project repository. Further Documentation Label Synchronization Process : Detailed explanation of how labels are synchronized. Silver Truth Generation Algorithm : Information on how the silver truth is computationally derived. Evaluation Strategy : Details on the metrics and approach used for evaluating results. Jupyter Notebooks Overview : A guide to the various analytical and utility notebooks. Project Roadmap : High-level overview of project phases, goals, and progress. References and Related Publications : Context for key research papers relevant to the project. Contact Issues and questions can be raised on GitHub.","title":"Home"},{"location":"#silver-truth-project-documentation","text":"","title":"Silver Truth Project Documentation"},{"location":"#introduction","text":"Welcome to the documentation for the Silver Truth project. This project focuses on generating and evaluating \"silver-standard\" corpora for cell tracking challenges. A Silver-standard corpus (silver truth) is defined as computer-generated reference annotations, obtained as the majority opinion over the results of several competitive algorithms submitted by former challenge participants. This corpus serves as a robust benchmark for evaluating new cell tracking algorithms.","title":"Introduction"},{"location":"#project-goal","text":"This project aims to create a new, better silver truth using more advanced techniques of Quality Assurance and Ensemble methods.","title":"Project Goal"},{"location":"#key-components-workflow","text":"The Silver Truth project is structured around a series of command-line tools that facilitate the entire process from data preparation to evaluation.","title":"Key Components &amp; Workflow"},{"location":"#high-level-workflow-diagram","text":"Raw Data -> Synchronization -> DataFrame -> Job Files -> Fusion -> Evaluation","title":"High-Level Workflow Diagram"},{"location":"#core-modules","text":"cli_preprocessing.py : Handles initial data preparation, including synchronizing datasets and creating structured dataframes. cli_fusion.py : Manages the generation of job files and the execution of the cell segmentation fusion process to create the silver truth. cli_evaluation.py : Provides tools for evaluating competitor algorithms against ground truth or the generated silver truth.","title":"Core Modules"},{"location":"#getting-started","text":"For detailed installation instructions and basic usage examples, please refer to the main README.md file in the project repository.","title":"Getting Started"},{"location":"#further-documentation","text":"Label Synchronization Process : Detailed explanation of how labels are synchronized. Silver Truth Generation Algorithm : Information on how the silver truth is computationally derived. Evaluation Strategy : Details on the metrics and approach used for evaluating results. Jupyter Notebooks Overview : A guide to the various analytical and utility notebooks. Project Roadmap : High-level overview of project phases, goals, and progress. References and Related Publications : Context for key research papers relevant to the project.","title":"Further Documentation"},{"location":"#contact","text":"Issues and questions can be raised on GitHub.","title":"Contact"},{"location":"Evaluations/","text":"Evaluation strategy While several evaluation methods exist to see the performance of the model, we will first evaluate the models, silver truth or otherwise on the basis of the following metrics: - Jaccard We currently have two places containing results that will help us see if we are evaluating the models correctly. Results from 2020.07.31 excel sheet, seg training page SEG_log files in the competitors' folders Utilizing synchronized data and dataset parquet from preprocessing step, we can now call evaluate competitor from evaluations.py This method outputs results of a selected competitor or all of them from a given dataset. These results when compared to SEG_log files and excel sheet are similar, but there are slight differences. To see these differences, we utilize the finding_problem_labels.ipynb in notebooks folder. This shows us that the synchronization algorithm we use is different to the one used in the competition. Slight differences in low percentage of labels skew the results a little bit. Here I'm going to make the decision to use our new synchronization, and therefore also it's results as our baseline.","title":"Evaluations"},{"location":"Evaluations/#evaluation-strategy","text":"While several evaluation methods exist to see the performance of the model, we will first evaluate the models, silver truth or otherwise on the basis of the following metrics: - Jaccard We currently have two places containing results that will help us see if we are evaluating the models correctly. Results from 2020.07.31 excel sheet, seg training page SEG_log files in the competitors' folders Utilizing synchronized data and dataset parquet from preprocessing step, we can now call evaluate competitor from evaluations.py This method outputs results of a selected competitor or all of them from a given dataset. These results when compared to SEG_log files and excel sheet are similar, but there are slight differences. To see these differences, we utilize the finding_problem_labels.ipynb in notebooks folder. This shows us that the synchronization algorithm we use is different to the one used in the competition. Slight differences in low percentage of labels skew the results a little bit. Here I'm going to make the decision to use our new synchronization, and therefore also it's results as our baseline.","title":"Evaluation strategy"},{"location":"References/","text":"References and Related Publications This section lists publications that are closely related to the concepts and techniques explored within this project, particularly concerning cell tracking, segmentation fusion, and quality assurance. These papers provide foundational knowledge and advanced methodologies relevant to the development of a new, improved silver truth dataset. Ensemble Deep Learning Object Detection Fusion for Cell Tracking Mitosis and Lineage.pdf Relevance : This paper presents an approach that utilizes an ensemble of deep learning models for robust object detection, coupled with a fusion strategy to enhance the precision and reliability of cell identification. This directly relates to the project's goal of using ensemble methods for a better silver truth. FAIA-325-FAIA200407.pdf Relevance : This paper focuses on fault diagnosis using a combination of AI techniques (fuzzy logic, ANNs, expert systems). While not directly about cell tracking, its exploration of robust and adaptive diagnostic frameworks using hybrid AI approaches could be relevant to the \"Quality Assurance\" aspect of generating a better silver truth. QANet_Quality_Assurance_Network_for_Micr.pdf Relevance : QANet introduces a deep learning framework to quantitatively estimate image segmentation quality without human inspection. This is highly relevant to the project's aim of using \"advanced techniques of Quality Assurance\" for the silver truth, as it addresses how to assess segmentation quality programmatically. Topology_Preserving_Segmentation_Fusion_For_Cells_With_Complex_Shapes.pdf Relevance : This paper describes a topology-preserving fusion method for merging complex shapes, important for establishing reference data for benchmarking segmentation algorithms. This directly informs the \"Ensemble methods\" and \"fusion\" aspects of generating silver truth, especially for challenging cell morphologies.","title":"References"},{"location":"References/#references-and-related-publications","text":"This section lists publications that are closely related to the concepts and techniques explored within this project, particularly concerning cell tracking, segmentation fusion, and quality assurance. These papers provide foundational knowledge and advanced methodologies relevant to the development of a new, improved silver truth dataset. Ensemble Deep Learning Object Detection Fusion for Cell Tracking Mitosis and Lineage.pdf Relevance : This paper presents an approach that utilizes an ensemble of deep learning models for robust object detection, coupled with a fusion strategy to enhance the precision and reliability of cell identification. This directly relates to the project's goal of using ensemble methods for a better silver truth. FAIA-325-FAIA200407.pdf Relevance : This paper focuses on fault diagnosis using a combination of AI techniques (fuzzy logic, ANNs, expert systems). While not directly about cell tracking, its exploration of robust and adaptive diagnostic frameworks using hybrid AI approaches could be relevant to the \"Quality Assurance\" aspect of generating a better silver truth. QANet_Quality_Assurance_Network_for_Micr.pdf Relevance : QANet introduces a deep learning framework to quantitatively estimate image segmentation quality without human inspection. This is highly relevant to the project's aim of using \"advanced techniques of Quality Assurance\" for the silver truth, as it addresses how to assess segmentation quality programmatically. Topology_Preserving_Segmentation_Fusion_For_Cells_With_Complex_Shapes.pdf Relevance : This paper describes a topology-preserving fusion method for merging complex shapes, important for establishing reference data for benchmarking segmentation algorithms. This directly informs the \"Ensemble methods\" and \"fusion\" aspects of generating silver truth, especially for challenging cell morphologies.","title":"References and Related Publications"},{"location":"Roadmap/","text":"Project Roadmap: Silver Truth - Beyond Recreation This roadmap outlines the strategic progression of the \"Silver Truth\" project, moving from the current recreation phase towards advanced research and development in Quality Assurance (QA) and Dynamic Ensemble methods. Status Key: * [ ] TODO : Not yet started. * [x] DONE : Completed. * [ ] IN PROGRESS : Currently being worked on. * [ ] ON HOLD : Temporarily paused. Phase 0: Current State - Silver Truth Recreation Goal: To accurately reproduce the existing silver truth generation process using the provided tools and data. [x] DONE Data synchronization ( cli_preprocessing.py synchronize-datasets ) Status : Completed. Details : Initial setup and verification of synchronization process. [x] DONE Dataset DataFrame creation ( cli_preprocessing.py create-dataset-dataframe ) Status : Completed. Details : Generation of .parquet files for downstream use. [ ] IN PROGRESS Job file generation ( cli_fusion.py generate-jobfiles ) Status : Currently implementing and testing. Details : [Link to Job File Generation Design Doc (TODO)] [ ] TODO Fusion execution ( cli_fusion.py run-fusion ) using the established CTC fusion algorithm Status : Pending. Details : [Link to Fusion Implementation Notes (TODO)] [ ] TODO Evaluation of recreated silver truth against known benchmarks ( cli_evaluation.py evaluate-competitor ) Status : Pending. Details : [Link to Evaluation Plan (TODO)] Phase 1: Novel QA-Enhanced Fusion (Core Research & Development) Goal: To develop and integrate a novel Quality Assurance (QA) network and a Dynamic Ensemble Architecture to improve the quality of fused segmentations by proactively filtering low-quality inputs. [ ] TODO Develop QA Network [ ] TODO Implement Frame-by-Frame QA Status : Research & Design. Challenges : [Link to QA Network Challenges (TODO)] [ ] TODO Implement Temporal QA (e.g., Temporal Consistency Scoring based on IoU between consecutive frames) Status : Research & Design. Challenges : [Link to Temporal QA Design (TODO)] [ ] TODO Develop Adaptive Thresholding for QA scores Status : Research & Design. Challenges : [Link to Adaptive Thresholding Strategy (TODO)] [ ] TODO Develop Dynamic Ensemble Architecture [ ] TODO Initial implementation with fixed-size inputs, exploring weight sharing and ignore masks for variable input handling Status : Research & Design. Challenges : [Link to Dynamic Ensemble Design (TODO)] [ ] TODO Investigate alternative approaches (e.g., recurrent networks, transformer-based, feature pooling) for handling variable input sizes and order invariance Status : Research & Design. Challenges : [Link to Variable Input Handling Research (TODO)] [ ] TODO Integrate QA and Dynamic Ensemble [ ] TODO Design and implement the multi-stage pipeline: Base Learners -> Synchronization -> QA Filter -> Ensemble -> Improved Segmentation Status : High-level design complete, detailed design pending. Details : [Link to Integrated Pipeline Architecture (TODO)] [ ] TODO Determine how to effectively pass QA scores as additional context to the ensemble Status : Research & Design. Details : [Link to QA Score Integration (TODO)] Phase 2: Comprehensive Evaluation and Validation Goal: To rigorously assess the performance of the novel QA-enhanced fusion method against state-of-the-art individual models and static ensemble methods. [ ] TODO Compare proposed method against CTC Baseline and best individual models using standard metrics (IoU, Dice Coefficient, F1 score) [ ] TODO Conduct evaluation on a separate, unseen test set [ ] TODO Perform ablation studies: Test each component (Frame-by-Frame QA, Temporal QA, Dynamic Ensemble) individually to quantify its specific impact on overall performance Phase 3: Application and Generalization Goal: To leverage the improved silver truth for training new models and explore the applicability of the developed methodologies to other computer vision domains. [ ] TODO Utilize the higher-quality silver truth data generated in Phase 1 to train new cell segmentation models [ ] TODO Evaluate the performance of models trained on the improved silver truth [ ] TODO Explore the applicability of the QA and dynamic ensemble techniques to other image segmentation tasks (e.g., medical imaging, autonomous driving) [ ] TODO Investigate potential applications beyond computer vision (e.g., dynamic simulation reweighting)","title":"Roadmap"},{"location":"Roadmap/#project-roadmap-silver-truth-beyond-recreation","text":"This roadmap outlines the strategic progression of the \"Silver Truth\" project, moving from the current recreation phase towards advanced research and development in Quality Assurance (QA) and Dynamic Ensemble methods. Status Key: * [ ] TODO : Not yet started. * [x] DONE : Completed. * [ ] IN PROGRESS : Currently being worked on. * [ ] ON HOLD : Temporarily paused.","title":"Project Roadmap: Silver Truth - Beyond Recreation"},{"location":"Roadmap/#phase-0-current-state-silver-truth-recreation","text":"Goal: To accurately reproduce the existing silver truth generation process using the provided tools and data. [x] DONE Data synchronization ( cli_preprocessing.py synchronize-datasets ) Status : Completed. Details : Initial setup and verification of synchronization process. [x] DONE Dataset DataFrame creation ( cli_preprocessing.py create-dataset-dataframe ) Status : Completed. Details : Generation of .parquet files for downstream use. [ ] IN PROGRESS Job file generation ( cli_fusion.py generate-jobfiles ) Status : Currently implementing and testing. Details : [Link to Job File Generation Design Doc (TODO)] [ ] TODO Fusion execution ( cli_fusion.py run-fusion ) using the established CTC fusion algorithm Status : Pending. Details : [Link to Fusion Implementation Notes (TODO)] [ ] TODO Evaluation of recreated silver truth against known benchmarks ( cli_evaluation.py evaluate-competitor ) Status : Pending. Details : [Link to Evaluation Plan (TODO)]","title":"Phase 0: Current State - Silver Truth Recreation"},{"location":"Roadmap/#phase-1-novel-qa-enhanced-fusion-core-research-development","text":"Goal: To develop and integrate a novel Quality Assurance (QA) network and a Dynamic Ensemble Architecture to improve the quality of fused segmentations by proactively filtering low-quality inputs. [ ] TODO Develop QA Network [ ] TODO Implement Frame-by-Frame QA Status : Research & Design. Challenges : [Link to QA Network Challenges (TODO)] [ ] TODO Implement Temporal QA (e.g., Temporal Consistency Scoring based on IoU between consecutive frames) Status : Research & Design. Challenges : [Link to Temporal QA Design (TODO)] [ ] TODO Develop Adaptive Thresholding for QA scores Status : Research & Design. Challenges : [Link to Adaptive Thresholding Strategy (TODO)] [ ] TODO Develop Dynamic Ensemble Architecture [ ] TODO Initial implementation with fixed-size inputs, exploring weight sharing and ignore masks for variable input handling Status : Research & Design. Challenges : [Link to Dynamic Ensemble Design (TODO)] [ ] TODO Investigate alternative approaches (e.g., recurrent networks, transformer-based, feature pooling) for handling variable input sizes and order invariance Status : Research & Design. Challenges : [Link to Variable Input Handling Research (TODO)] [ ] TODO Integrate QA and Dynamic Ensemble [ ] TODO Design and implement the multi-stage pipeline: Base Learners -> Synchronization -> QA Filter -> Ensemble -> Improved Segmentation Status : High-level design complete, detailed design pending. Details : [Link to Integrated Pipeline Architecture (TODO)] [ ] TODO Determine how to effectively pass QA scores as additional context to the ensemble Status : Research & Design. Details : [Link to QA Score Integration (TODO)]","title":"Phase 1: Novel QA-Enhanced Fusion (Core Research &amp; Development)"},{"location":"Roadmap/#phase-2-comprehensive-evaluation-and-validation","text":"Goal: To rigorously assess the performance of the novel QA-enhanced fusion method against state-of-the-art individual models and static ensemble methods. [ ] TODO Compare proposed method against CTC Baseline and best individual models using standard metrics (IoU, Dice Coefficient, F1 score) [ ] TODO Conduct evaluation on a separate, unseen test set [ ] TODO Perform ablation studies: Test each component (Frame-by-Frame QA, Temporal QA, Dynamic Ensemble) individually to quantify its specific impact on overall performance","title":"Phase 2: Comprehensive Evaluation and Validation"},{"location":"Roadmap/#phase-3-application-and-generalization","text":"Goal: To leverage the improved silver truth for training new models and explore the applicability of the developed methodologies to other computer vision domains. [ ] TODO Utilize the higher-quality silver truth data generated in Phase 1 to train new cell segmentation models [ ] TODO Evaluate the performance of models trained on the improved silver truth [ ] TODO Explore the applicability of the QA and dynamic ensemble techniques to other image segmentation tasks (e.g., medical imaging, autonomous driving) [ ] TODO Investigate potential applications beyond computer vision (e.g., dynamic simulation reweighting)","title":"Phase 3: Application and Generalization"},{"location":"Silver-truth-generation/","text":"Silver Truth Generation Algorithm The \"silver truth\" in this project refers to a computer-generated reference annotation, derived from the majority opinion of multiple competitive cell tracking algorithms. This process is crucial for creating robust benchmarks for evaluating new algorithms in cell tracking challenges. Overview of the Fusion Process The generation of silver truth is primarily handled by the cli_fusion.py script, which acts as a Python wrapper around a powerful Java-based fusion tool, specifically the Annotations Fusing tools plugin (packaged as a standalone JAR file). This approach allows us to leverage the robust fusion capabilities of the Java tool while providing a convenient command-line interface through Python. The general workflow for generating silver truth involves: Preparing Input Data : Ensuring that synchronized segmentation results from various competitor algorithms are available. Generating Job Files : Creating a job specification file that lists the input image patterns for the fusion tool. This is done using the generate-jobfiles command within cli_fusion.py . Running the Fusion Tool : Executing the Java fusion tool via the run-fusion command in cli_fusion.py , providing the job file and other necessary parameters. The cli_fusion.py Script The cli_fusion.py script provides two main commands: 1. generate-jobfiles This command prepares the input for the fusion process by creating a job specification file. This file tells the fusion tool which images from which competitors to consider for fusion. Usage: python cli_fusion.py generate-jobfiles --parquet-file <path_to_parquet_file> --campaign-number <campaign_number> --output-dir <output_directory> [OPTIONS] Key Parameters: --parquet-file : Path to the Parquet dataset file (e.g., BF-C2DL-HSC_dataset_dataframe.parquet ), which contains metadata about the synchronized datasets and competitor results. --campaign-number : A unique identifier for the current fusion campaign (e.g., '01' ). --output-dir : Directory where the generated job file will be saved. --tracking-marker-column (optional): The column name in the parquet file that contains the tracking marker paths (default: tracking_markers ). --competitor-columns (optional, can be specified multiple times): Column names in the parquet file that contain competitor result paths. If not provided, all columns except known non-competitor columns will be considered. Example: python cli_fusion.py generate-jobfiles --parquet-file BF-C2DL-HSC_dataset_dataframe.parquet --campaign-number 01 --output-dir job_files 2. run-fusion This command executes the actual segmentation fusion process using the Java JAR. It takes the generated job file and other configuration parameters to produce the fused silver truth segmentations. Usage: python cli_fusion.py run-fusion --jar-path <path_to_jar> --job-file <path_to_job_file> --output-pattern <output_pattern> --time-points <time_points> --num-threads <num_threads> --model <model> [OPTIONS] Key Parameters: --jar-path (required): Path to the executable Java JAR file (e.g., fusers-all-dependencies.jar ). This JAR encapsulates the Annotations Fusing tools . --job-file (required): Path to the job specification file generated by generate-jobfiles . --output-pattern (required): Output filename pattern for the fused images, including TTT or TTTT placeholders for time points (e.g., /path/to/fused_TTT.tif ). --time-points (required): A string specifying the time points to process (e.g., \"1-9,23,25\" ). --num-threads (required): Number of processing threads to use for the fusion. --model (required): The fusion model to use. Available models are defined by the FusionModel enum in src/fusion/fusion.py (e.g., weighted_average , majority_vote ). --threshold (optional): Voting threshold for merging (default: 1.0 ). --cmv-mode (optional): Enable Combinatorial Model Validation mode (e.g., \"CMV\" , \"CMV2_8\" ). --seg-folder (optional): Optional path to ground truth folder for scoring during fusion. --debug (optional): Enable debug logging and show Java process output. Example: python cli_fusion.py run-fusion --jar-path src/data_processing/cell_tracking_java_helpers/label-fusion-ng-2.2.0-SNAPSHOT-jar-with-dependencies.jar --job-file job_files/BF-C2DL-HSC_01_job_file.txt --output-pattern data/fused/BF-C2DL-HSC_fused_TTT.tif --time-points \"1-10\" --num-threads 4 --model \"weighted_average\" Fusion Models The --model parameter in run-fusion allows selection of different fusion algorithms. These correspond to the FusionModel enum in src/fusion/fusion.py . Common models include: weighted_average majority_vote Inputs Required by the Java Plugin The underlying Java Annotations Fusing tools plugin primarily requires: Input Segmentation Images : These are the individual segmentation results from various competitor algorithms, typically in .tif format. Job Specification File : A text file (generated by generate-jobfiles ) that lists the paths to these input images for each time point and competitor. Outputs of the Fusion Process The run-fusion command outputs the fused segmentation images, which represent the generated silver truth. These are typically .tif files following the specified --output-pattern .","title":"Silver-truth Generation"},{"location":"Silver-truth-generation/#silver-truth-generation-algorithm","text":"The \"silver truth\" in this project refers to a computer-generated reference annotation, derived from the majority opinion of multiple competitive cell tracking algorithms. This process is crucial for creating robust benchmarks for evaluating new algorithms in cell tracking challenges.","title":"Silver Truth Generation Algorithm"},{"location":"Silver-truth-generation/#overview-of-the-fusion-process","text":"The generation of silver truth is primarily handled by the cli_fusion.py script, which acts as a Python wrapper around a powerful Java-based fusion tool, specifically the Annotations Fusing tools plugin (packaged as a standalone JAR file). This approach allows us to leverage the robust fusion capabilities of the Java tool while providing a convenient command-line interface through Python. The general workflow for generating silver truth involves: Preparing Input Data : Ensuring that synchronized segmentation results from various competitor algorithms are available. Generating Job Files : Creating a job specification file that lists the input image patterns for the fusion tool. This is done using the generate-jobfiles command within cli_fusion.py . Running the Fusion Tool : Executing the Java fusion tool via the run-fusion command in cli_fusion.py , providing the job file and other necessary parameters.","title":"Overview of the Fusion Process"},{"location":"Silver-truth-generation/#the-cli_fusionpy-script","text":"The cli_fusion.py script provides two main commands:","title":"The cli_fusion.py Script"},{"location":"Silver-truth-generation/#1-generate-jobfiles","text":"This command prepares the input for the fusion process by creating a job specification file. This file tells the fusion tool which images from which competitors to consider for fusion. Usage: python cli_fusion.py generate-jobfiles --parquet-file <path_to_parquet_file> --campaign-number <campaign_number> --output-dir <output_directory> [OPTIONS] Key Parameters: --parquet-file : Path to the Parquet dataset file (e.g., BF-C2DL-HSC_dataset_dataframe.parquet ), which contains metadata about the synchronized datasets and competitor results. --campaign-number : A unique identifier for the current fusion campaign (e.g., '01' ). --output-dir : Directory where the generated job file will be saved. --tracking-marker-column (optional): The column name in the parquet file that contains the tracking marker paths (default: tracking_markers ). --competitor-columns (optional, can be specified multiple times): Column names in the parquet file that contain competitor result paths. If not provided, all columns except known non-competitor columns will be considered. Example: python cli_fusion.py generate-jobfiles --parquet-file BF-C2DL-HSC_dataset_dataframe.parquet --campaign-number 01 --output-dir job_files","title":"1. generate-jobfiles"},{"location":"Silver-truth-generation/#2-run-fusion","text":"This command executes the actual segmentation fusion process using the Java JAR. It takes the generated job file and other configuration parameters to produce the fused silver truth segmentations. Usage: python cli_fusion.py run-fusion --jar-path <path_to_jar> --job-file <path_to_job_file> --output-pattern <output_pattern> --time-points <time_points> --num-threads <num_threads> --model <model> [OPTIONS] Key Parameters: --jar-path (required): Path to the executable Java JAR file (e.g., fusers-all-dependencies.jar ). This JAR encapsulates the Annotations Fusing tools . --job-file (required): Path to the job specification file generated by generate-jobfiles . --output-pattern (required): Output filename pattern for the fused images, including TTT or TTTT placeholders for time points (e.g., /path/to/fused_TTT.tif ). --time-points (required): A string specifying the time points to process (e.g., \"1-9,23,25\" ). --num-threads (required): Number of processing threads to use for the fusion. --model (required): The fusion model to use. Available models are defined by the FusionModel enum in src/fusion/fusion.py (e.g., weighted_average , majority_vote ). --threshold (optional): Voting threshold for merging (default: 1.0 ). --cmv-mode (optional): Enable Combinatorial Model Validation mode (e.g., \"CMV\" , \"CMV2_8\" ). --seg-folder (optional): Optional path to ground truth folder for scoring during fusion. --debug (optional): Enable debug logging and show Java process output. Example: python cli_fusion.py run-fusion --jar-path src/data_processing/cell_tracking_java_helpers/label-fusion-ng-2.2.0-SNAPSHOT-jar-with-dependencies.jar --job-file job_files/BF-C2DL-HSC_01_job_file.txt --output-pattern data/fused/BF-C2DL-HSC_fused_TTT.tif --time-points \"1-10\" --num-threads 4 --model \"weighted_average\"","title":"2. run-fusion"},{"location":"Silver-truth-generation/#fusion-models","text":"The --model parameter in run-fusion allows selection of different fusion algorithms. These correspond to the FusionModel enum in src/fusion/fusion.py . Common models include: weighted_average majority_vote","title":"Fusion Models"},{"location":"Silver-truth-generation/#inputs-required-by-the-java-plugin","text":"The underlying Java Annotations Fusing tools plugin primarily requires: Input Segmentation Images : These are the individual segmentation results from various competitor algorithms, typically in .tif format. Job Specification File : A text file (generated by generate-jobfiles ) that lists the paths to these input images for each time point and competitor.","title":"Inputs Required by the Java Plugin"},{"location":"Silver-truth-generation/#outputs-of-the-fusion-process","text":"The run-fusion command outputs the fused segmentation images, which represent the generated silver truth. These are typically .tif files following the specified --output-pattern .","title":"Outputs of the Fusion Process"},{"location":"ensemble/","text":"Ensemble This section presents a study on different processes intended to replace the Fusion step on the silver-truth workflow. The objective is to find the best performant process capable of generating an accurate segmentation of an image of a cell given several proposed segmentations. Deliverables Strategies Specific startegies are related to different datasets. Quantifying the error from learning just from gt; Transfer learning, starting from gt; A layered sequence of new syntetic (slightly improved) dataset generation for training a new model; this can be done with single segmentation (and a single model) and also normalized segmentations (with multiple models per layer); for inferece, the input will follow through the sequence of trained models; Adding gt to the input, while training, for the model to learn not to modify such segmentation; Simple competitors normalization; Normalizations alongside the raw image; Basic segmentation for comparison; Use QA and non-QA databanks for comparison; Data Concepts: Databank is the folder where the images are gathered and the corresponding parquet file. Dataset is the specific data structure used for training models. Dataset versions A1 : gt -> gt [1 input-> 1 output], ground truth to ground truth; can be used for an initial training stage before transfer to another dataset; does this strategy offer benefits? A2 : gt&raw -> gt [2->1], ground truth along side raw image to ground truth; for other [2->1] models. B1 : seg -> gt [1->1], segmentation to ground truth; can we improve the images? If so, it may be used to create a syntetic dataset which can be used to train a new model. B2 : seg&raw -> gt [2->1], segmentation along side raw image to ground truth; B3 : seg+gt -> gt [1->1], segmentation and ground truth to ground truth; does adding some ground truths improve the results, compared with dataset B? C1 : norm_seg -> gt [1->1], competitors normalized segmentation to ground truth. C2 : norm_seg&raw -> gt [2->1], competitors normalized segmentation along side raw image to ground truth; same as above but with an additional input; may have some issues in cases where the raw images contain additional cells; does it improve performance upon dataset D? D1 : raw -> gt [1->1], raw image to ground truth; may have more trouble with additional cells in the images - will have to learn to ignore everything around the centered structure; what's the center of the image? This external information is already given by centering the image in the intended cell (so it's not a completely raw dataset); for ablation study to understand how much competitors segmentations improve our models. Steps: Create QA and non-QA (for ablation study) databanks: each databank folder has its corresponding parquet file on the same directory; each parquet file contains, among other information: a checksum for each file/image; the name of each file. the parquet files are saved in github/mlflow (?). Create Ensemble databank: for each ground truth, create the input series (a sequence of images corresponding to the same cell: norm_seg, seg1, seg2...): normalized competitors segmentations with ground truth, suffix: \"_normseg\"; layers RGB: [norm_seg, gt, raw]; single competitors segmentation with ground truth, suffix: \"_seg[N]\"; layers RGB: [seg, gt, raw]; each segmentation/raw image is centered according to the normalized segmentation image of the same series; the parquet file also contains the name and checksum of each file; the parquet files are saved in github/mlflow (?). Compute a split of the Ensemble parquet indices according to the different datasets: each resulting set (train, val, test) must contain the same input series as the other sets of the same type; any set of Dataset [X], whether with multiple files with the same ground truth or not, have direct correspondance to any set of the same type of a different Dataset. example 1: all sets of Dataset A1 contain the same files as the Dataset C1; example 2: all sets of Dataset B1 contains multiple files with the same ground truth, and in a direct correspondance with all sets of dataset A1. steps: I) load Ensemble parquet to a dataframe; II) select only norm_seg images; III) do a random split of the indices, according to a given seed value; training (70%), validation (15%) and test(15%); log the split details; IV) create 3 dataframe with the content of the splitted indices; these dataframes are used for the different sets of Dataset A1 and C1; V) use this dataframe to create the other sets, following the same input series location; Instantiate the different datasets sets, with the corresponding dataframe versions. calculate the checksum for each image and assert with the corresponding checksum in the dataframe; Models Training Data augmentation Results","title":"Ensemble"},{"location":"ensemble/#ensemble","text":"This section presents a study on different processes intended to replace the Fusion step on the silver-truth workflow. The objective is to find the best performant process capable of generating an accurate segmentation of an image of a cell given several proposed segmentations.","title":"Ensemble"},{"location":"ensemble/#deliverables","text":"","title":"Deliverables"},{"location":"ensemble/#strategies","text":"Specific startegies are related to different datasets. Quantifying the error from learning just from gt; Transfer learning, starting from gt; A layered sequence of new syntetic (slightly improved) dataset generation for training a new model; this can be done with single segmentation (and a single model) and also normalized segmentations (with multiple models per layer); for inferece, the input will follow through the sequence of trained models; Adding gt to the input, while training, for the model to learn not to modify such segmentation; Simple competitors normalization; Normalizations alongside the raw image; Basic segmentation for comparison; Use QA and non-QA databanks for comparison;","title":"Strategies"},{"location":"ensemble/#data","text":"","title":"Data"},{"location":"ensemble/#concepts","text":"Databank is the folder where the images are gathered and the corresponding parquet file. Dataset is the specific data structure used for training models.","title":"Concepts:"},{"location":"ensemble/#dataset-versions","text":"A1 : gt -> gt [1 input-> 1 output], ground truth to ground truth; can be used for an initial training stage before transfer to another dataset; does this strategy offer benefits? A2 : gt&raw -> gt [2->1], ground truth along side raw image to ground truth; for other [2->1] models. B1 : seg -> gt [1->1], segmentation to ground truth; can we improve the images? If so, it may be used to create a syntetic dataset which can be used to train a new model. B2 : seg&raw -> gt [2->1], segmentation along side raw image to ground truth; B3 : seg+gt -> gt [1->1], segmentation and ground truth to ground truth; does adding some ground truths improve the results, compared with dataset B? C1 : norm_seg -> gt [1->1], competitors normalized segmentation to ground truth. C2 : norm_seg&raw -> gt [2->1], competitors normalized segmentation along side raw image to ground truth; same as above but with an additional input; may have some issues in cases where the raw images contain additional cells; does it improve performance upon dataset D? D1 : raw -> gt [1->1], raw image to ground truth; may have more trouble with additional cells in the images - will have to learn to ignore everything around the centered structure; what's the center of the image? This external information is already given by centering the image in the intended cell (so it's not a completely raw dataset); for ablation study to understand how much competitors segmentations improve our models.","title":"Dataset versions"},{"location":"ensemble/#steps","text":"Create QA and non-QA (for ablation study) databanks: each databank folder has its corresponding parquet file on the same directory; each parquet file contains, among other information: a checksum for each file/image; the name of each file. the parquet files are saved in github/mlflow (?). Create Ensemble databank: for each ground truth, create the input series (a sequence of images corresponding to the same cell: norm_seg, seg1, seg2...): normalized competitors segmentations with ground truth, suffix: \"_normseg\"; layers RGB: [norm_seg, gt, raw]; single competitors segmentation with ground truth, suffix: \"_seg[N]\"; layers RGB: [seg, gt, raw]; each segmentation/raw image is centered according to the normalized segmentation image of the same series; the parquet file also contains the name and checksum of each file; the parquet files are saved in github/mlflow (?). Compute a split of the Ensemble parquet indices according to the different datasets: each resulting set (train, val, test) must contain the same input series as the other sets of the same type; any set of Dataset [X], whether with multiple files with the same ground truth or not, have direct correspondance to any set of the same type of a different Dataset. example 1: all sets of Dataset A1 contain the same files as the Dataset C1; example 2: all sets of Dataset B1 contains multiple files with the same ground truth, and in a direct correspondance with all sets of dataset A1. steps: I) load Ensemble parquet to a dataframe; II) select only norm_seg images; III) do a random split of the indices, according to a given seed value; training (70%), validation (15%) and test(15%); log the split details; IV) create 3 dataframe with the content of the splitted indices; these dataframes are used for the different sets of Dataset A1 and C1; V) use this dataframe to create the other sets, following the same input series location; Instantiate the different datasets sets, with the corresponding dataframe versions. calculate the checksum for each image and assert with the corresponding checksum in the dataframe;","title":"Steps:"},{"location":"ensemble/#models","text":"","title":"Models"},{"location":"ensemble/#training","text":"","title":"Training"},{"location":"ensemble/#data-augmentation","text":"","title":"Data augmentation"},{"location":"ensemble/#results","text":"","title":"Results"},{"location":"experiment_tracking/","text":"Experiment Tracking This project uses a combination of DVC (Data Version Control) and MLflow to ensure that all experiments are reproducible, trackable, and comparable. Core Tools MLflow : Used for logging experiment parameters, metrics, and artifacts. It provides a web-based UI to visualize and compare results across different runs. DVC (Data Version Control) : Used to version datasets and large models without bloating the Git repository. It ensures that we can always trace back to the exact version of the data used in any experiment. Workflow The primary entry point for conducting experiments is the run_experiment.py script. Running an Experiment To run an experiment, use the following command: python run_experiment.py [OPTIONS] This script will: Start an MLflow Run : A new run is created under a specified experiment name (default: silver-truth-qa ). Log Parameters : All command-line options and configuration parameters are logged to MLflow. Execute the Pipeline : The script orchestrates the necessary steps, such as running the fusion process ( cli_fusion.py ) and evaluating the results ( cli_evaluation.py ). Log Results : Key metrics (e.g., Jaccard scores) and output artifacts (e.g., evaluation CSVs, sample images) are logged to MLflow for analysis. Viewing Results To launch the MLflow UI and view the results of your experiments, run: mlflow ui This will start a local web server. You can navigate to the displayed URL (usually http://127.0.0.1:5000 ) in your browser to see the tracking dashboard. Data Versioning The data/ directory is tracked by DVC. When changes are made to the dataset (e.g., new preprocessing steps), you can version the new data using: dvc add data git commit data.dvc -m \"Updated dataset with new preprocessing\" This ensures that each Git commit is linked to a specific, versioned state of the data.","title":"Experiment Tracking"},{"location":"experiment_tracking/#experiment-tracking","text":"This project uses a combination of DVC (Data Version Control) and MLflow to ensure that all experiments are reproducible, trackable, and comparable.","title":"Experiment Tracking"},{"location":"experiment_tracking/#core-tools","text":"MLflow : Used for logging experiment parameters, metrics, and artifacts. It provides a web-based UI to visualize and compare results across different runs. DVC (Data Version Control) : Used to version datasets and large models without bloating the Git repository. It ensures that we can always trace back to the exact version of the data used in any experiment.","title":"Core Tools"},{"location":"experiment_tracking/#workflow","text":"The primary entry point for conducting experiments is the run_experiment.py script.","title":"Workflow"},{"location":"experiment_tracking/#running-an-experiment","text":"To run an experiment, use the following command: python run_experiment.py [OPTIONS] This script will: Start an MLflow Run : A new run is created under a specified experiment name (default: silver-truth-qa ). Log Parameters : All command-line options and configuration parameters are logged to MLflow. Execute the Pipeline : The script orchestrates the necessary steps, such as running the fusion process ( cli_fusion.py ) and evaluating the results ( cli_evaluation.py ). Log Results : Key metrics (e.g., Jaccard scores) and output artifacts (e.g., evaluation CSVs, sample images) are logged to MLflow for analysis.","title":"Running an Experiment"},{"location":"experiment_tracking/#viewing-results","text":"To launch the MLflow UI and view the results of your experiments, run: mlflow ui This will start a local web server. You can navigate to the displayed URL (usually http://127.0.0.1:5000 ) in your browser to see the tracking dashboard.","title":"Viewing Results"},{"location":"experiment_tracking/#data-versioning","text":"The data/ directory is tracked by DVC. When changes are made to the dataset (e.g., new preprocessing steps), you can version the new data using: dvc add data git commit data.dvc -m \"Updated dataset with new preprocessing\" This ensures that each Git commit is linked to a specific, versioned state of the data.","title":"Data Versioning"},{"location":"label_synchronizer/","text":"Label Synchronization Process Overview This documentation describes the process of synchronizing labels between silver truth, competitors, ground truth, and tracking files using standalone cell tracking challenge Java application. Note: We are no longer using a separate Fiji installation (except for when packaged inside the JAR file). The entire synchronization functionality is now contained within the JAR file. Prerequisites A Java Runtime Environment (JRE) installed. The standalone JAR file for the 'Annotation Label Sync2' plugin, which contains all necessary dependencies. Input data organized in the dataset structure. Large amount of disk space for the output data, the synchronizer does not compress the data. Obtaining the JAR File To generate the JAR file, follow these steps: Step 1: Clone the Repository Clone the repository containing the source code for the 'Annotation Label Sync2' plugin. git clone https://github.com/David-Ciz/label-fusion-ng-fork Step 2: Build the JAR File Navigate to the root directory of the cloned repository and run the following command to build the JAR file: mvn clean package The JAR file will be generated in the target directory. Running the Synchronization Using python interface You can use the provided Python wrapper script to run the synchronization process in the preprocessing.py file. The script reads the configuration file and runs the synchronization process using the standalone JAR. python cli_preprocessing.py synchronize_dataset data/inputs-2020-07 data/synchronize_data Notes: - Replace data/inputs-2020-07 with the path to the input data directory (it should contain folders with datasets). - Replace data/synchronize_data with the path to the output directory. Using the Command Line Since we have moved the synchronization functionality into a standalone JAR file, the synchronization can now be run via a standard Java command without needing a separate Fiji installation. Use the following command: java -cp target/LabelSyncer2Runner-1.0-SNAPSHOT-jar-with-dependencies.jar de.mpicbg.ulman.fusion.RunLabelSyncer2 /absolute/path/to/segmentation/results /absolute/path/to/tra/markers /absolute/path/to/output Notes: - Ensure that all paths provided are absolute. - Replace /absolute/path/to/segmentation/results , /absolute/path/to/tra/markers , and /absolute/path/to/output with the actual paths on your system. - If you prefer, you can move the JAR to a convenient directory and adjust the command accordingly. - It is highly recommended to compress the output data after synchronization to save disk space. Results The synchronized data will be saved in the specified output directory with the following structure: output_dir/ \u251c\u2500\u2500 Dataset1/ \u2502 \u251c\u2500\u2500 Competitor1/ \u2502 \u2502 \u251c\u2500\u2500 t001.tif \u2502 \u251c\u2500\u2500 Competitor2/ \u2502 \u2502 \u251c\u2500\u2500 t001.tif \u2502 \u2514\u2500\u2500 ... Validation To verify a successful synchronization: - Check that the number of output files matches the input. - Verify that labels are consistent across temporal sequences. - Confirm that the metadata.json file contains the expected information. Common Issues and Solutions Missing Output Files: Possible Cause: Incorrect input paths. Solution: Verify that the input directories are correct and contain valid data. Inconsistent Labels: Possible Cause: TRA markers mismatch. Solution: Check that the TRA marker files follow the expected format. Plugin Error: Possible Cause: Java version mismatch or dependency issues. Solution: Ensure you are using a compatible Java version and that all dependencies are included in the JAR file. Version History Date Version Changes Author 2025-03-27 1.0 Initial documentation update David \u010c\u00ed\u017e References Java Documentation Annotation Label Sync2 Plugin Documentation Related Publications https://github.com/CellTrackingChallenge/label-fusion-ng/tree/master ```markdown","title":"Label Synchronizer"},{"location":"label_synchronizer/#label-synchronization-process","text":"","title":"Label Synchronization Process"},{"location":"label_synchronizer/#overview","text":"This documentation describes the process of synchronizing labels between silver truth, competitors, ground truth, and tracking files using standalone cell tracking challenge Java application. Note: We are no longer using a separate Fiji installation (except for when packaged inside the JAR file). The entire synchronization functionality is now contained within the JAR file.","title":"Overview"},{"location":"label_synchronizer/#prerequisites","text":"A Java Runtime Environment (JRE) installed. The standalone JAR file for the 'Annotation Label Sync2' plugin, which contains all necessary dependencies. Input data organized in the dataset structure. Large amount of disk space for the output data, the synchronizer does not compress the data.","title":"Prerequisites"},{"location":"label_synchronizer/#obtaining-the-jar-file","text":"To generate the JAR file, follow these steps:","title":"Obtaining the JAR File"},{"location":"label_synchronizer/#step-1-clone-the-repository","text":"Clone the repository containing the source code for the 'Annotation Label Sync2' plugin. git clone https://github.com/David-Ciz/label-fusion-ng-fork","title":"Step 1: Clone the Repository"},{"location":"label_synchronizer/#step-2-build-the-jar-file","text":"Navigate to the root directory of the cloned repository and run the following command to build the JAR file: mvn clean package The JAR file will be generated in the target directory.","title":"Step 2: Build the JAR File"},{"location":"label_synchronizer/#running-the-synchronization","text":"","title":"Running the Synchronization"},{"location":"label_synchronizer/#using-python-interface","text":"You can use the provided Python wrapper script to run the synchronization process in the preprocessing.py file. The script reads the configuration file and runs the synchronization process using the standalone JAR. python cli_preprocessing.py synchronize_dataset data/inputs-2020-07 data/synchronize_data Notes: - Replace data/inputs-2020-07 with the path to the input data directory (it should contain folders with datasets). - Replace data/synchronize_data with the path to the output directory.","title":"Using python interface"},{"location":"label_synchronizer/#using-the-command-line","text":"Since we have moved the synchronization functionality into a standalone JAR file, the synchronization can now be run via a standard Java command without needing a separate Fiji installation. Use the following command: java -cp target/LabelSyncer2Runner-1.0-SNAPSHOT-jar-with-dependencies.jar de.mpicbg.ulman.fusion.RunLabelSyncer2 /absolute/path/to/segmentation/results /absolute/path/to/tra/markers /absolute/path/to/output Notes: - Ensure that all paths provided are absolute. - Replace /absolute/path/to/segmentation/results , /absolute/path/to/tra/markers , and /absolute/path/to/output with the actual paths on your system. - If you prefer, you can move the JAR to a convenient directory and adjust the command accordingly. - It is highly recommended to compress the output data after synchronization to save disk space.","title":"Using the Command Line"},{"location":"label_synchronizer/#results","text":"The synchronized data will be saved in the specified output directory with the following structure: output_dir/ \u251c\u2500\u2500 Dataset1/ \u2502 \u251c\u2500\u2500 Competitor1/ \u2502 \u2502 \u251c\u2500\u2500 t001.tif \u2502 \u251c\u2500\u2500 Competitor2/ \u2502 \u2502 \u251c\u2500\u2500 t001.tif \u2502 \u2514\u2500\u2500 ...","title":"Results"},{"location":"label_synchronizer/#validation","text":"To verify a successful synchronization: - Check that the number of output files matches the input. - Verify that labels are consistent across temporal sequences. - Confirm that the metadata.json file contains the expected information.","title":"Validation"},{"location":"label_synchronizer/#common-issues-and-solutions","text":"Missing Output Files: Possible Cause: Incorrect input paths. Solution: Verify that the input directories are correct and contain valid data. Inconsistent Labels: Possible Cause: TRA markers mismatch. Solution: Check that the TRA marker files follow the expected format. Plugin Error: Possible Cause: Java version mismatch or dependency issues. Solution: Ensure you are using a compatible Java version and that all dependencies are included in the JAR file.","title":"Common Issues and Solutions"},{"location":"label_synchronizer/#version-history","text":"Date Version Changes Author 2025-03-27 1.0 Initial documentation update David \u010c\u00ed\u017e","title":"Version History"},{"location":"label_synchronizer/#references","text":"Java Documentation Annotation Label Sync2 Plugin Documentation Related Publications https://github.com/CellTrackingChallenge/label-fusion-ng/tree/master ```markdown","title":"References"},{"location":"qa_data_preparation/","text":"QA Model Data Preparation and Splitting Strategy For the Quality Assurance (QA) model, a specific data preparation and splitting strategy is required to ensure valid, reproducible results. 1. Data Preparation: From Whole Images to Cell Crops The QA model is trained at the cell level, not the whole-image level. A preprocessing script is used to convert the raw images and competitor segmentations into a dataset suitable for training. This script performs the following steps: 1. It iterates through every competitor segmentation for every raw image. 2. For each individual cell in a segmentation, it calculates a bounding box (e.g., 64x64 pixels). 3. It uses this same bounding box to create two corresponding image crops: one from the raw image and one from the competitor's segmentation mask . 4. These two crops are stacked into a single, 2-channel image file and saved. This stacked image is the direct input for the QA model. 5. A cell_level_dataframe.parquet file is generated to hold the metadata for every stacked crop. The resulting dataframe has the following structure: cell_id stacked_crop_path original_image_key competitor cell_jaccard 00001 data/cell_crops/00001.tif BF-C2DL-HSC/01 competitor_A 0.92 00002 data/cell_crops/00002.tif BF-C2DL-HSC/01 competitor_B 0.65 ... ... ... ... ... Dataflow Diagram graph TD A[Raw Images] --> C{Preprocessing Script}; B[Competitor Segmentations] --> C; C --> D[Cell-Level Crops]; D --> E[2-Channel Stacked Images]; E --> F[cell_level_dataframe.parquet]; subgraph \"Data Preparation\" A B C D E F end G[cell_level_dataframe.parquet] --> H{Splitting Script}; H --> I[train/validation/test splits]; I --> J[cell_level_dataframe_with_splits.parquet]; subgraph \"Data Splitting\" G H I J end K[cell_level_dataframe_with_splits.parquet] --> L(QA Model Training); M[Stacked Crop Images] --> L; subgraph \"Model Training\" K M L end 2. Data Splitting: Ensuring No Data Leakage To create valid training, validation, and test sets, we must prevent data leakage. The golden rule is: all cells that come from the same original source image must belong to the same split. The splitting process is as follows: 1. Generate the cell_level_dataframe.parquet as described above. 2. Get a list of all unique original_image_key values from the dataframe. 3. Shuffle this list of keys using a fixed random seed. 4. Split the shuffled list of keys into training, validation, and test sets (e.g., 70%/15%/15%). 5. Create a new split column in the dataframe. Assign 'train' , 'validation' , or 'test' to each cell based on which set its original_image_key belongs to. 3. Data Versioning with DVC The final, split dataframe ( cell_level_dataframe_with_splits.parquet ) and the directory of stacked crop images are versioned with DVC. dvc add data/cell_crops data/cell_level_dataframe_with_splits.parquet git add . git commit -m \"feat: Create versioned QA dataset with splits\" This ensures that every experiment can be rerun on the exact same data by checking out the corresponding Git commit and running dvc pull .","title":"QA Data Preparation"},{"location":"qa_data_preparation/#qa-model-data-preparation-and-splitting-strategy","text":"For the Quality Assurance (QA) model, a specific data preparation and splitting strategy is required to ensure valid, reproducible results.","title":"QA Model Data Preparation and Splitting Strategy"},{"location":"qa_data_preparation/#1-data-preparation-from-whole-images-to-cell-crops","text":"The QA model is trained at the cell level, not the whole-image level. A preprocessing script is used to convert the raw images and competitor segmentations into a dataset suitable for training. This script performs the following steps: 1. It iterates through every competitor segmentation for every raw image. 2. For each individual cell in a segmentation, it calculates a bounding box (e.g., 64x64 pixels). 3. It uses this same bounding box to create two corresponding image crops: one from the raw image and one from the competitor's segmentation mask . 4. These two crops are stacked into a single, 2-channel image file and saved. This stacked image is the direct input for the QA model. 5. A cell_level_dataframe.parquet file is generated to hold the metadata for every stacked crop. The resulting dataframe has the following structure: cell_id stacked_crop_path original_image_key competitor cell_jaccard 00001 data/cell_crops/00001.tif BF-C2DL-HSC/01 competitor_A 0.92 00002 data/cell_crops/00002.tif BF-C2DL-HSC/01 competitor_B 0.65 ... ... ... ... ...","title":"1. Data Preparation: From Whole Images to Cell Crops"},{"location":"qa_data_preparation/#dataflow-diagram","text":"graph TD A[Raw Images] --> C{Preprocessing Script}; B[Competitor Segmentations] --> C; C --> D[Cell-Level Crops]; D --> E[2-Channel Stacked Images]; E --> F[cell_level_dataframe.parquet]; subgraph \"Data Preparation\" A B C D E F end G[cell_level_dataframe.parquet] --> H{Splitting Script}; H --> I[train/validation/test splits]; I --> J[cell_level_dataframe_with_splits.parquet]; subgraph \"Data Splitting\" G H I J end K[cell_level_dataframe_with_splits.parquet] --> L(QA Model Training); M[Stacked Crop Images] --> L; subgraph \"Model Training\" K M L end","title":"Dataflow Diagram"},{"location":"qa_data_preparation/#2-data-splitting-ensuring-no-data-leakage","text":"To create valid training, validation, and test sets, we must prevent data leakage. The golden rule is: all cells that come from the same original source image must belong to the same split. The splitting process is as follows: 1. Generate the cell_level_dataframe.parquet as described above. 2. Get a list of all unique original_image_key values from the dataframe. 3. Shuffle this list of keys using a fixed random seed. 4. Split the shuffled list of keys into training, validation, and test sets (e.g., 70%/15%/15%). 5. Create a new split column in the dataframe. Assign 'train' , 'validation' , or 'test' to each cell based on which set its original_image_key belongs to.","title":"2. Data Splitting: Ensuring No Data Leakage"},{"location":"qa_data_preparation/#3-data-versioning-with-dvc","text":"The final, split dataframe ( cell_level_dataframe_with_splits.parquet ) and the directory of stacked crop images are versioned with DVC. dvc add data/cell_crops data/cell_level_dataframe_with_splits.parquet git add . git commit -m \"feat: Create versioned QA dataset with splits\" This ensures that every experiment can be rerun on the exact same data by checking out the corresponding Git commit and running dvc pull .","title":"3. Data Versioning with DVC"},{"location":"quality_assurance/","text":"Quality Assurance (QA) This section outlines a proposed methodology for implementing a Quality Assurance (QA) step into the silver-truth generation pipeline. The goal of the QA process is to identify and potentially exclude low-quality segmentations from the fusion process, thereby improving the final fused output. QA Model We propose a model that takes a raw source image and a competitor's segmentation as input and predicts the quality of the segmentation. This model can be implemented in two ways: Classification Model: Outputs a binary classification of \"good\" or \"bad\" for a given segmentation. Regression Model: Outputs a continuous value, such as the Jaccard Index, representing the quality of the segmentation. This QA model can be applied at two different granularities: Image Level: The model assesses the overall quality of a segmentation for an entire image. Cell Level: The model assesses the quality of the segmentation for individual cells within an image. Experimental Framework To evaluate the effectiveness of the QA model, we will test three different strategies for incorporating its output into the fusion pipeline. These strategies involve excluding data at different levels of granularity based on the QA model's predictions and a variety of confidence thresholds. 1. Competitor-Level Exclusion In this approach, if a competitor's segmentations are consistently rated as \"bad\" by the QA model across multiple images, the entire competitor is excluded from the fusion process. 2. Image-Level Exclusion This strategy involves excluding specific images from a competitor. If the QA model flags a particular segmentation from a competitor on a specific image as \"bad\", that single image segmentation is excluded from the fusion process, while the competitor's other segmentations are still used. 3. Cell-Level Exclusion This is the most granular approach. The QA model identifies individual cell segmentations within an image that are of low quality. These specific \"bad\" cells are then removed from the fusion process, while the rest of the cells from that competitor's segmentation are retained. By experimenting with these three strategies and adjusting the thresholds for what constitutes a \"bad\" segmentation, we can determine the most effective way to leverage the QA model to improve the quality of the final silver-truth annotations. Limitations and Future Work It is important to note that this initial proposal is a first, naive approach. It has several limitations that should be addressed in future work: Error Types: The current model does not explicitly differentiate between different types of errors, such as false positives and false negatives. A more sophisticated model would be able to identify not just that a segmentation is bad, but why (e.g., missing a cell vs. hallucinating a cell). Temporal Information: The proposed model is static and does not incorporate temporal information from video sequences. Future iterations should leverage temporal consistency to improve the accuracy of the QA process. For example, a cell that is correctly identified in preceding and succeeding frames is more likely to be a true positive.","title":"Quality Assurance"},{"location":"quality_assurance/#quality-assurance-qa","text":"This section outlines a proposed methodology for implementing a Quality Assurance (QA) step into the silver-truth generation pipeline. The goal of the QA process is to identify and potentially exclude low-quality segmentations from the fusion process, thereby improving the final fused output.","title":"Quality Assurance (QA)"},{"location":"quality_assurance/#qa-model","text":"We propose a model that takes a raw source image and a competitor's segmentation as input and predicts the quality of the segmentation. This model can be implemented in two ways: Classification Model: Outputs a binary classification of \"good\" or \"bad\" for a given segmentation. Regression Model: Outputs a continuous value, such as the Jaccard Index, representing the quality of the segmentation. This QA model can be applied at two different granularities: Image Level: The model assesses the overall quality of a segmentation for an entire image. Cell Level: The model assesses the quality of the segmentation for individual cells within an image.","title":"QA Model"},{"location":"quality_assurance/#experimental-framework","text":"To evaluate the effectiveness of the QA model, we will test three different strategies for incorporating its output into the fusion pipeline. These strategies involve excluding data at different levels of granularity based on the QA model's predictions and a variety of confidence thresholds.","title":"Experimental Framework"},{"location":"quality_assurance/#1-competitor-level-exclusion","text":"In this approach, if a competitor's segmentations are consistently rated as \"bad\" by the QA model across multiple images, the entire competitor is excluded from the fusion process.","title":"1. Competitor-Level Exclusion"},{"location":"quality_assurance/#2-image-level-exclusion","text":"This strategy involves excluding specific images from a competitor. If the QA model flags a particular segmentation from a competitor on a specific image as \"bad\", that single image segmentation is excluded from the fusion process, while the competitor's other segmentations are still used.","title":"2. Image-Level Exclusion"},{"location":"quality_assurance/#3-cell-level-exclusion","text":"This is the most granular approach. The QA model identifies individual cell segmentations within an image that are of low quality. These specific \"bad\" cells are then removed from the fusion process, while the rest of the cells from that competitor's segmentation are retained. By experimenting with these three strategies and adjusting the thresholds for what constitutes a \"bad\" segmentation, we can determine the most effective way to leverage the QA model to improve the quality of the final silver-truth annotations.","title":"3. Cell-Level Exclusion"},{"location":"quality_assurance/#limitations-and-future-work","text":"It is important to note that this initial proposal is a first, naive approach. It has several limitations that should be addressed in future work: Error Types: The current model does not explicitly differentiate between different types of errors, such as false positives and false negatives. A more sophisticated model would be able to identify not just that a segmentation is bad, but why (e.g., missing a cell vs. hallucinating a cell). Temporal Information: The proposed model is static and does not incorporate temporal information from video sequences. Future iterations should leverage temporal consistency to improve the accuracy of the QA process. For example, a cell that is correctly identified in preceding and succeeding frames is more likely to be a true positive.","title":"Limitations and Future Work"},{"location":"datasets/structure/","text":"Cell Tracking Challenge Dataset Structure Overview The project contains 16 distinct biological datasets from the Cell Tracking Challenge, each featuring different cell types and imaging modalities. Each dataset follows a standardized structure with multiple annotations and results. Dataset Categories Dataset ID Description Modality BF-C2DL-HSC Hematopoietic Stem Cells Brightfield 2D BF-C2DL-MuSC Muscle Stem Cells Brightfield 2D DIC-C2DH-HeLa HeLa Cells DIC 2D Fluo-C2DL-MSC Mesenchymal Stem Cells Fluorescence 2D Fluo-C3DH-A549 Lung Cancer Cells Fluorescence 3D Fluo-C3DH-A549-SIM Simulated Lung Cancer Cells Fluorescence 3D Fluo-C3DH-H157 Lung Cancer Cells Fluorescence 3D Fluo-C3DL-MDA231 Breast Cancer Cells Fluorescence 3D Fluo-N2DH-GOWT1 Embryonic Stem Cells Fluorescence 2D Fluo-N2DH-SIM+ Simulated Nuclei Fluorescence 2D Fluo-N2DL-HeLa HeLa Cell Nuclei Fluorescence 2D Fluo-N3DH-CE C. elegans Embryo Nuclei Fluorescence 3D Fluo-N3DH-CHO Chinese Hamster Ovary Nuclei Fluorescence 3D Fluo-N3DH-SIM+ Simulated Nuclei Fluorescence 3D PhC-C2DH-U373 Glioblastoma-astrocytoma Cells Phase Contrast 2D PhC-C2DL-PSC Pancreatic Stem Cells Phase Contrast 2D Directory Structure Each dataset follows this structure: Dataset/ \u251c\u2500\u2500 01/ # First sequence raw images \u251c\u2500\u2500 01_GT/ # Ground truth for first sequence \u2502 \u251c\u2500\u2500 SEG/ # Segmentation ground truth (sparse) \u2502 \u2514\u2500\u2500 TRA/ # Tracking ground truth (point annotations) \u251c\u2500\u2500 01_GT_sync/ # Synchronized ground truth \u251c\u2500\u2500 01_ST/ # Silver truth for first sequence \u251c\u2500\u2500 01_ST_sync/ # Synchronized silver truth \u251c\u2500\u2500 02/ # Second sequence raw images \u251c\u2500\u2500 02_GT/ # Ground truth for second sequence \u2502 \u251c\u2500\u2500 SEG/ # Segmentation ground truth (sparse) \u2502 \u2514\u2500\u2500 TRA/ # Tracking ground truth (point annotations) \u251c\u2500\u2500 02_GT_sync/ # Synchronized ground truth \u251c\u2500\u2500 02_ST/ # Silver truth for second sequence \u251c\u2500\u2500 02_ST_sync/ # Synchronized silver truth \u2514\u2500\u2500 Competitors/ # Competitor results \u251c\u2500\u2500 CALT-US/ \u2502 \u251c\u2500\u2500 01_RES/ # Results for first sequence \u2502 \u2514\u2500\u2500 02_RES/ # Results for second sequence \u251c\u2500\u2500 DREX-US/ \u251c\u2500\u2500 KIT-Sch-GE/ \u251c\u2500\u2500 KTH-SE/ \u2514\u2500\u2500 MU-Lux-CZ/ Data Types Raw Images (01/ and 02/) Format: TIFF files Content: Original microscopy images Two independent sequences per dataset Ground Truth (01_GT/ and 02_GT/) Contains two types of annotations: SEG/ (Segmentation) Sparse manual segmentations by human experts TIFF format Full cell masks at selected time points Used for segmentation accuracy evaluation TRA/ (Tracking) Point annotations for cell tracking TIFF format Single point per cell in each frame Used for tracking accuracy evaluation Silver Truth (01_ST/ and 02_ST/) Combined best results from competition participants Generated through consensus of top-performing methods TIFF format Provides more dense annotations than ground truth Synchronized Versions (*_sync/) Processed versions of GT and ST Ensures consistent labeling across temporal sequences Maintains label correspondence between frames TIFF format Competitor Results Each competitor folder contains: - 01_RES/: Results for first sequence - 02_RES/: Results for second sequence - All results in TIFF format - Each competitor uses their own methodology File Format Specifications All images are stored in TIFF format with the following characteristics: - Segmentation masks: Binary or labeled images where each cell has a unique identifier - Tracking markers: Binary images with single points marking cell positions - Raw images: Original microscopy data in the respective modality Usage Notes When working with the dataset, always maintain the original directory structure Synchronization between SEG and TRA files is crucial for accurate evaluation Silver truth can be used as a more extensive ground truth alternative Each sequence (01 and 02) should be processed independently Related Documentation Label Synchronization Process Evaluation Metrics","title":"Cell Tracking Challenge Dataset Structure"},{"location":"datasets/structure/#cell-tracking-challenge-dataset-structure","text":"","title":"Cell Tracking Challenge Dataset Structure"},{"location":"datasets/structure/#overview","text":"The project contains 16 distinct biological datasets from the Cell Tracking Challenge, each featuring different cell types and imaging modalities. Each dataset follows a standardized structure with multiple annotations and results.","title":"Overview"},{"location":"datasets/structure/#dataset-categories","text":"Dataset ID Description Modality BF-C2DL-HSC Hematopoietic Stem Cells Brightfield 2D BF-C2DL-MuSC Muscle Stem Cells Brightfield 2D DIC-C2DH-HeLa HeLa Cells DIC 2D Fluo-C2DL-MSC Mesenchymal Stem Cells Fluorescence 2D Fluo-C3DH-A549 Lung Cancer Cells Fluorescence 3D Fluo-C3DH-A549-SIM Simulated Lung Cancer Cells Fluorescence 3D Fluo-C3DH-H157 Lung Cancer Cells Fluorescence 3D Fluo-C3DL-MDA231 Breast Cancer Cells Fluorescence 3D Fluo-N2DH-GOWT1 Embryonic Stem Cells Fluorescence 2D Fluo-N2DH-SIM+ Simulated Nuclei Fluorescence 2D Fluo-N2DL-HeLa HeLa Cell Nuclei Fluorescence 2D Fluo-N3DH-CE C. elegans Embryo Nuclei Fluorescence 3D Fluo-N3DH-CHO Chinese Hamster Ovary Nuclei Fluorescence 3D Fluo-N3DH-SIM+ Simulated Nuclei Fluorescence 3D PhC-C2DH-U373 Glioblastoma-astrocytoma Cells Phase Contrast 2D PhC-C2DL-PSC Pancreatic Stem Cells Phase Contrast 2D","title":"Dataset Categories"},{"location":"datasets/structure/#directory-structure","text":"Each dataset follows this structure: Dataset/ \u251c\u2500\u2500 01/ # First sequence raw images \u251c\u2500\u2500 01_GT/ # Ground truth for first sequence \u2502 \u251c\u2500\u2500 SEG/ # Segmentation ground truth (sparse) \u2502 \u2514\u2500\u2500 TRA/ # Tracking ground truth (point annotations) \u251c\u2500\u2500 01_GT_sync/ # Synchronized ground truth \u251c\u2500\u2500 01_ST/ # Silver truth for first sequence \u251c\u2500\u2500 01_ST_sync/ # Synchronized silver truth \u251c\u2500\u2500 02/ # Second sequence raw images \u251c\u2500\u2500 02_GT/ # Ground truth for second sequence \u2502 \u251c\u2500\u2500 SEG/ # Segmentation ground truth (sparse) \u2502 \u2514\u2500\u2500 TRA/ # Tracking ground truth (point annotations) \u251c\u2500\u2500 02_GT_sync/ # Synchronized ground truth \u251c\u2500\u2500 02_ST/ # Silver truth for second sequence \u251c\u2500\u2500 02_ST_sync/ # Synchronized silver truth \u2514\u2500\u2500 Competitors/ # Competitor results \u251c\u2500\u2500 CALT-US/ \u2502 \u251c\u2500\u2500 01_RES/ # Results for first sequence \u2502 \u2514\u2500\u2500 02_RES/ # Results for second sequence \u251c\u2500\u2500 DREX-US/ \u251c\u2500\u2500 KIT-Sch-GE/ \u251c\u2500\u2500 KTH-SE/ \u2514\u2500\u2500 MU-Lux-CZ/","title":"Directory Structure"},{"location":"datasets/structure/#data-types","text":"","title":"Data Types"},{"location":"datasets/structure/#raw-images-01-and-02","text":"Format: TIFF files Content: Original microscopy images Two independent sequences per dataset","title":"Raw Images (01/ and 02/)"},{"location":"datasets/structure/#ground-truth-01_gt-and-02_gt","text":"Contains two types of annotations: SEG/ (Segmentation) Sparse manual segmentations by human experts TIFF format Full cell masks at selected time points Used for segmentation accuracy evaluation TRA/ (Tracking) Point annotations for cell tracking TIFF format Single point per cell in each frame Used for tracking accuracy evaluation","title":"Ground Truth (01_GT/ and 02_GT/)"},{"location":"datasets/structure/#silver-truth-01_st-and-02_st","text":"Combined best results from competition participants Generated through consensus of top-performing methods TIFF format Provides more dense annotations than ground truth","title":"Silver Truth (01_ST/ and 02_ST/)"},{"location":"datasets/structure/#synchronized-versions-_sync","text":"Processed versions of GT and ST Ensures consistent labeling across temporal sequences Maintains label correspondence between frames TIFF format","title":"Synchronized Versions (*_sync/)"},{"location":"datasets/structure/#competitor-results","text":"Each competitor folder contains: - 01_RES/: Results for first sequence - 02_RES/: Results for second sequence - All results in TIFF format - Each competitor uses their own methodology","title":"Competitor Results"},{"location":"datasets/structure/#file-format-specifications","text":"All images are stored in TIFF format with the following characteristics: - Segmentation masks: Binary or labeled images where each cell has a unique identifier - Tracking markers: Binary images with single points marking cell positions - Raw images: Original microscopy data in the respective modality","title":"File Format Specifications"},{"location":"datasets/structure/#usage-notes","text":"When working with the dataset, always maintain the original directory structure Synchronization between SEG and TRA files is crucial for accurate evaluation Silver truth can be used as a more extensive ground truth alternative Each sequence (01 and 02) should be processed independently","title":"Usage Notes"},{"location":"datasets/structure/#related-documentation","text":"Label Synchronization Process Evaluation Metrics","title":"Related Documentation"},{"location":"development%20notes/label_synchronizer/","text":"27.3 In sync Synchronization seems to work, might benefit from optimization, but whatever Verification of synchronization created, synchronization seems to be working properly Going to work with the first dataset Created the Goals document, could be improved, but to me it's clear that the next step should be to recreate the evaluations, in general. Everything will be so much simpler, if it has a clear structure. I propose a pandas dataframe that has soruce, gt, tm, competitor, 01/02 adding it to the preprocessing 31.3 Out of space My Mac ran out of space for the synchronized images, turns out, the files are saved uncompressed, while the originals are compressed. Generated a code to replace the original synchronized images with their compressed versions. Going to try and modify the label synchronized algorithm. 248 0.123664 but SEG_log has GT_label=31 J=0.0 for 02 campaign, image 02_1585.tif BF-C2DL-HSC DREX US Why is that? I see overlap in them, seems to me my calculation is correct","title":"Label synchronizer"},{"location":"development%20notes/label_synchronizer/#273-in-sync","text":"Synchronization seems to work, might benefit from optimization, but whatever Verification of synchronization created, synchronization seems to be working properly Going to work with the first dataset Created the Goals document, could be improved, but to me it's clear that the next step should be to recreate the evaluations, in general. Everything will be so much simpler, if it has a clear structure. I propose a pandas dataframe that has soruce, gt, tm, competitor, 01/02 adding it to the preprocessing","title":"27.3 In sync"},{"location":"development%20notes/label_synchronizer/#313-out-of-space","text":"My Mac ran out of space for the synchronized images, turns out, the files are saved uncompressed, while the originals are compressed. Generated a code to replace the original synchronized images with their compressed versions. Going to try and modify the label synchronized algorithm. 248 0.123664 but SEG_log has GT_label=31 J=0.0 for 02 campaign, image 02_1585.tif BF-C2DL-HSC DREX US Why is that? I see overlap in them, seems to me my calculation is correct","title":"31.3 Out of space"},{"location":"goals/general_goals_overview/","text":"General goal of the project Generating a new silver truth is part of a bigger project of MLEM-QA - Machine Learning using Ensemble Methods and Quality Assurance. The project aims to improve the quality of the silver truth. To achieve this goal, we want to explore two main directions: 1. QA part - Improving the selection criteria for the silver truth. 2. EM part - Improving the algorithm for generating the silver truth. The Setup To be able to evaluate the effect of our modifications, we need to have a baseline. The metric to improve is silver truth quality, which is evaluated ???? We were provided with a competition that contains 16 datasets. recreated silver truth needs to match the exisiting silver truth ???? synchronize labels >> validate synchronization >> recreate evaluation of competitors >> generate silver truth >> evaluate silver truth >> improve silver truth","title":"General goal of the project"},{"location":"goals/general_goals_overview/#general-goal-of-the-project","text":"Generating a new silver truth is part of a bigger project of MLEM-QA - Machine Learning using Ensemble Methods and Quality Assurance. The project aims to improve the quality of the silver truth. To achieve this goal, we want to explore two main directions: 1. QA part - Improving the selection criteria for the silver truth. 2. EM part - Improving the algorithm for generating the silver truth.","title":"General goal of the project"},{"location":"goals/general_goals_overview/#the-setup","text":"To be able to evaluate the effect of our modifications, we need to have a baseline. The metric to improve is silver truth quality, which is evaluated ???? We were provided with a competition that contains 16 datasets. recreated silver truth needs to match the exisiting silver truth ???? synchronize labels >> validate synchronization >> recreate evaluation of competitors >> generate silver truth >> evaluate silver truth >> improve silver truth","title":"The Setup"}]}